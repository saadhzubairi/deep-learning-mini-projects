\begin{thebibliography}{10}

\bibitem{belinkov2018synthetic}
Y.~Belinkov and Y.~Bisk.
\newblock Synthetic and natural noise both break neural machine translation.
\newblock In {\em ICLR}, 2018.

\bibitem{cubuk2019autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In {\em CVPR}, pages 113--123, 2019.

\bibitem{howard2018universal}
J.~Howard and S.~Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock In {\em ACL}, pages 328--339, 2018.

\bibitem{hu2022lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem{iyyer2015deep}
M.~Iyyer, A.~Manjunatha, J.~Boyd-Graber, and H.~D. III.
\newblock Deep unordered composition rivals syntactic methods for text
  classification.
\newblock In {\em ACL}, pages 1681--1691, 2015.

\bibitem{kovaleva2019revealing}
O.~Kovaleva, A.~Romanov, A.~Rogers, and A.~Rumshisky.
\newblock Revealing the dark secrets of bert.
\newblock In {\em EMNLP-IJCNLP}, pages 4365--4374, 2019.

\bibitem{li2023lora}
S.~Li, Y.~Yang, Y.~Shen, F.~Wei, Z.~Lu, L.~Qiu, and Y.~Yang.
\newblock Expressive and generalizable low-rank adaptation for large models via
  slow cascaded learning, 2024.

\bibitem{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis,
  L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em JMLR}, 15(56):1929--1958, 2014.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision, 2015.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, 30, 2017.

\bibitem{wei2019eda}
J.~Wei and K.~Zou.
\newblock Eda: Easy data augmentation techniques for boosting performance on
  text classification tasks.
\newblock In {\em EMNLP-IJCNLP}, 2019.

\bibitem{zhang2015character}
X.~Zhang, J.~Zhao, and Y.~LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em NeurIPS}, volume~28, 2015.

\end{thebibliography}
