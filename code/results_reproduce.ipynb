{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets evaluate > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Standard Library ───────────────────────────────────────────────────────────\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import json\n",
    "import zipfile\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "# ── Third‑Party Libraries ─────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "\n",
    "# ── PyTorch ────────────────────────────────────────────────────────────────────\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# ── Hugging Face Transformers ─────────────────────────────────────────────────\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    "    logging as hf_logging\n",
    ")\n",
    "\n",
    "# ── Hugging Face Datasets ─────────────────────────────────────────────────────\n",
    "from datasets import Dataset as HFDataset, load_dataset\n",
    "\n",
    "# ── PEFT (Parameter‑Efficient Fine‑Tuning) ─────────────────────────────────────\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    PeftConfig,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model,\n",
    ")\n",
    "\n",
    "# ── Safetensors ────────────────────────────────────────────────────────────────\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# ── Logging Configuration ─────────────────────────────────────────────────────\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "# hf_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kaggle credentials\n",
    "kaggle_creds = {\n",
    "    \"username\": \"hurryingauto3\",\n",
    "    \"key\": \"17e33c07cfd0993aecbc770b33c7054e\"\n",
    "}\n",
    "\n",
    "# Ensure the Kaggle config directory exists\n",
    "os.makedirs(os.path.expanduser(\"~/.config/kaggle/\"), exist_ok=True)\n",
    "\n",
    "# Write credentials to kaggle.json\n",
    "with open(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), \"w\") as f:\n",
    "    json.dump(kaggle_creds, f)\n",
    "\n",
    "# Set correct permissions\n",
    "os.chmod(os.path.expanduser(\"~/.config/kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "# Remove the \"data/\" directory if it exists\n",
    "os.system(\"rm -rf data/\")\n",
    "\n",
    "# --- Kaggle API ---\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(text, n=1):\n",
    "    \"\"\"Replace n words in the sentence with their synonyms using WordNet.\"\"\"\n",
    "    words = text.split()\n",
    "    new_words = words.copy()\n",
    "    # Filter out short words and potentially common stop words if desired\n",
    "    random_word_list = list(set([word for word in words if len(word) > 3])) # Keep simple filter\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:\n",
    "        synonyms = wordnet.synsets(random_word)\n",
    "        if synonyms:\n",
    "            # Get lemmas from the first synset\n",
    "            syn_words = [lemma.name() for lemma in synonyms[0].lemmas()]\n",
    "            # Filter out the original word itself if it appears as a synonym\n",
    "            syn_words = [s.replace('_', ' ') for s in syn_words if s.lower() != random_word.lower()]\n",
    "            if syn_words:\n",
    "                synonym = random.choice(syn_words) # Choose a random synonym\n",
    "                # More robust replacement (case-insensitive match)\n",
    "                for i, word in enumerate(new_words):\n",
    "                    if word.lower() == random_word.lower():\n",
    "                        new_words[i] = synonym\n",
    "                num_replaced += 1\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def word_dropout(text, dropout_prob=0.1):\n",
    "    \"\"\"Randomly drops words with a given probability.\"\"\"\n",
    "    words = text.split()\n",
    "    if not words: return \"\" # Handle empty strings\n",
    "    # Keep at least one word if dropout_prob is high\n",
    "    survivors = [word for word in words if random.random() > dropout_prob]\n",
    "    if not survivors and words: # Ensure at least one word remains if original wasn't empty\n",
    "        return random.choice(words)\n",
    "    return \" \".join(survivors)\n",
    "\n",
    "def random_swap(text, n=1):\n",
    "    \"\"\"Randomly swaps two words n times.\"\"\"\n",
    "    words = text.split()\n",
    "    length = len(words)\n",
    "    if length < 2: # Cannot swap if less than 2 words\n",
    "        return text\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(length), 2)\n",
    "        words[idx1], words[idx2] = words[idx2], words[idx1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def add_noise(text, noise_prob=0.05):\n",
    "    \"\"\"Add various types of random noise to words.\"\"\"\n",
    "    def corrupt(word):\n",
    "        if random.random() > noise_prob or not word:\n",
    "            return word\n",
    "\n",
    "        # Choose a random character index (safer for substitution/insertion)\n",
    "        char_idx = random.randint(0, len(word) - 1) if len(word) > 0 else 0\n",
    "        insert_idx = random.randint(0, len(word))\n",
    "\n",
    "        # More balanced noise types\n",
    "        noise_type = random.choice(['substitute', 'insert_char', 'delete_char', 'swap_char', 'repeat_char', 'capitalize'])\n",
    "\n",
    "        if noise_type == 'substitute' and len(word) > 0:\n",
    "            return word[:char_idx] + random.choice(string.ascii_lowercase) + word[char_idx+1:]\n",
    "        elif noise_type == 'insert_char':\n",
    "             return word[:insert_idx] + random.choice(string.ascii_lowercase + string.punctuation) + word[insert_idx:]\n",
    "        elif noise_type == 'delete_char' and len(word) > 0:\n",
    "             return word[:char_idx] + word[char_idx+1:]\n",
    "        elif noise_type == 'swap_char' and len(word) > 1:\n",
    "             swap_idx = random.randint(0, len(word) - 2)\n",
    "             word_list = list(word)\n",
    "             word_list[swap_idx], word_list[swap_idx+1] = word_list[swap_idx+1], word_list[swap_idx]\n",
    "             return \"\".join(word_list)\n",
    "        elif noise_type == 'repeat_char' and len(word) > 0:\n",
    "            return word[:insert_idx] + word[char_idx] + word[insert_idx:] # Repeat random char\n",
    "        elif noise_type == 'capitalize':\n",
    "            return ''.join(c.upper() if random.random() < 0.5 else c.lower() for c in word) # Random case changes\n",
    "\n",
    "        return word # Fallback\n",
    "\n",
    "    # Apply corruption word by word, filter out potential empty strings from deletion\n",
    "    return \" \".join(filter(None, [corrupt(word) for word in text.split()]))\n",
    "\n",
    "\n",
    "# Define your preferred default augmentation settings here\n",
    "def rich_text_augment(\n",
    "    text,\n",
    "    synonym_prob=0.15,\n",
    "    dropout_prob=0.05,\n",
    "    swap_prob=0.05,\n",
    "    noise_prob=0.0 # Noise was disabled in your last run\n",
    "):\n",
    "    \"\"\"Apply multiple augmentation strategies with given probabilities.\"\"\"\n",
    "    # Apply augmentations sequentially, allowing multiple per text\n",
    "    if random.random() < synonym_prob:\n",
    "        text = synonym_replacement(text, n=1) # Limit to 1 synonym replacement\n",
    "    if random.random() < dropout_prob:\n",
    "        text = word_dropout(text, dropout_prob)\n",
    "    if random.random() < swap_prob:\n",
    "        text = random_swap(text, n=1)\n",
    "    if random.random() < noise_prob:\n",
    "        text = add_noise(text, noise_prob)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom Dataset for the Competition Test File ---\n",
    "class AGNewsTestDataset(Dataset):\n",
    "    def __init__(self, pkl_file, tokenizer, max_length=512, text_column=\"text\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.text_column = text_column\n",
    "        self.texts = []\n",
    "\n",
    "        try:\n",
    "            with open(pkl_file, 'rb') as f:\n",
    "                loaded = pickle.load(f)\n",
    "\n",
    "            if HFDataset and isinstance(loaded, HFDataset):\n",
    "                if self.text_column in loaded.column_names:\n",
    "                    self.texts = loaded[self.text_column]\n",
    "                else:\n",
    "                    raise ValueError(f\"Missing column '{self.text_column}'; cols: {loaded.column_names}\")\n",
    "            elif isinstance(loaded, list):\n",
    "                self.texts = loaded\n",
    "            elif isinstance(loaded, dict):\n",
    "                for key in ('text','data','description'):\n",
    "                    if key in loaded and isinstance(loaded[key], list):\n",
    "                        self.texts = loaded[key]\n",
    "                        break\n",
    "                else:\n",
    "                    raise ValueError(f\"No list field found; keys: {list(loaded.keys())}\")\n",
    "            else:\n",
    "                raise TypeError(f\"Unsupported pickle type: {type(loaded)}\")\n",
    "\n",
    "            if not self.texts:\n",
    "                raise ValueError(f\"No text data extracted from {pkl_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed to load test pickle '%s': %s\", pkl_file, e)\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        text = str(text)\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        out = {k: v for k,v in enc.items() if k != 'token_type_ids'}\n",
    "        out['index'] = idx\n",
    "        return out\n",
    "\n",
    "\n",
    "class AGNewsDataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path=\"roberta-base\",\n",
    "        data_dir=\"./data_agnews\",\n",
    "        competition_name=\"deep-learning-spring-2025-project-2\",\n",
    "        batch_size=32,\n",
    "        test_batch_size=32,\n",
    "        num_workers=2,\n",
    "        max_seq_length=512,\n",
    "        val_split_percentage=0.0,\n",
    "        filter_max_words=256,\n",
    "        filter_max_nonalpha_ratio=0.1,\n",
    "        apply_augmentation=True,\n",
    "    ):\n",
    "        self.model_name_or_path = model_name_or_path\n",
    "        self.data_dir = data_dir\n",
    "        self.competition_name = competition_name\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.val_split_percentage = val_split_percentage\n",
    "        self.filter_max_words = filter_max_words\n",
    "        self.filter_max_nonalpha_ratio = filter_max_nonalpha_ratio\n",
    "        self.apply_augmentation = apply_augmentation\n",
    "\n",
    "        self.competition_path = os.path.join(self.data_dir, self.competition_name)\n",
    "        self.zip_path = os.path.join(self.competition_path, f\"{self.competition_name}.zip\")\n",
    "        self.test_pkl = os.path.join(self.competition_path, \"test_unlabelled.pkl\")\n",
    "        self.hf_cache_dir = os.path.join(self.data_dir, \"hf_cache\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)\n",
    "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.predict_dataset = None\n",
    "\n",
    "    def _keep_example(self, ex):\n",
    "        if self.filter_max_words and len(ex[\"text\"].split()) > self.filter_max_words:\n",
    "            return False\n",
    "        if self.filter_max_nonalpha_ratio is not None:\n",
    "            nonalpha = sum(1 for c in ex[\"text\"] if not c.isalnum() and not c.isspace())\n",
    "            if nonalpha / max(1,len(ex[\"text\"])) > self.filter_max_nonalpha_ratio:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _tokenize_function_train(self, examples):\n",
    "        texts = examples[\"text\"]\n",
    "        if self.apply_augmentation:\n",
    "            texts = [rich_text_augment(t) for t in texts]\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_seq_length,\n",
    "        )\n",
    "\n",
    "    def _tokenize_function_eval(self, examples):\n",
    "        return self.tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_seq_length,\n",
    "        )\n",
    "\n",
    "    def prepare_data(self):\n",
    "        load_dataset(\"ag_news\", cache_dir=self.hf_cache_dir, trust_remote_code=True)\n",
    "        self.download_competition_data()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage in (None, \"fit\",\"validate\"):\n",
    "            ds = load_dataset(\"ag_news\", cache_dir=self.hf_cache_dir, trust_remote_code=True)\n",
    "            train_raw, test_raw = ds['train'], ds['test']\n",
    "\n",
    "            if self.val_split_percentage>0:\n",
    "                split = train_raw.train_test_split(test_size=self.val_split_percentage, seed=42)\n",
    "                train_src, val_src = split['train'], split['test']\n",
    "            else:\n",
    "                train_src, val_src = train_raw, test_raw\n",
    "\n",
    "            train_f = train_src.filter(self._keep_example, num_proc=self.num_workers)\n",
    "            self.train_dataset = train_f.map(\n",
    "                self._tokenize_function_train, batched=True,\n",
    "                remove_columns=[\"text\"], num_proc=self.num_workers\n",
    "            )\n",
    "            self.train_dataset.set_format(\"torch\")\n",
    "\n",
    "            val_t = val_src.map(\n",
    "                self._tokenize_function_eval, batched=True,\n",
    "                remove_columns=[\"text\"], num_proc=self.num_workers\n",
    "            )\n",
    "            self.val_dataset = val_t\n",
    "            self.val_dataset.set_format(\"torch\")\n",
    "\n",
    "        if stage in (None, \"test\"):\n",
    "            if not self.predict_dataset:\n",
    "                self.predict_dataset = AGNewsTestDataset(\n",
    "                    self.test_pkl, self.tokenizer, self.max_seq_length\n",
    "                )\n",
    "\n",
    "    def get_train_loader(self):\n",
    "        if not self.train_dataset: self.setup(\"fit\")\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.data_collator,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "    def get_val_loader(self):\n",
    "        if not self.val_dataset: self.setup(\"validate\")\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.test_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.data_collator,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "    def get_competition_test_loader(self):\n",
    "        if not self.predict_dataset: self.setup(\"test\")\n",
    "        return DataLoader(\n",
    "            self.predict_dataset,\n",
    "            batch_size=self.test_batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=self.data_collator,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "    def download_competition_data(self):\n",
    "        if not os.path.exists(self.test_pkl):\n",
    "            os.makedirs(self.competition_path, exist_ok=True)\n",
    "            try:\n",
    "                from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "                api = KaggleApi(); api.authenticate()\n",
    "                api.competition_download_files(self.competition_name, path=self.competition_path)\n",
    "                if os.path.exists(self.zip_path):\n",
    "                    with zipfile.ZipFile(self.zip_path, 'r') as z: z.extractall(self.competition_path)\n",
    "                    os.remove(self.zip_path)\n",
    "                else:\n",
    "                    log.warning(\"Zip not found after download: %s\", self.zip_path)\n",
    "            except ImportError:\n",
    "                log.warning(\"Kaggle API missing; download manually to %s\", self.test_pkl)\n",
    "            except Exception as e:\n",
    "                log.error(\"Kaggle download/extract failed: %s\", e)\n",
    "\n",
    "        if not os.path.exists(self.test_pkl):\n",
    "            raise FileNotFoundError(f\"Missing test file: {self.test_pkl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d751bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration\n",
    "MODEL_ID = \"roberta-base\"\n",
    "COMPETITION_ID = \"deep-learning-spring-2025-project-2\" # Double-check this ID\n",
    "DATA_DIR = \"./agnews_data\"\n",
    "BATCH_SIZE = 32\n",
    "TEST_BATCH_SIZE = 32\n",
    "\n",
    "# Instantiate the data module\n",
    "data_module = AGNewsDataModule(\n",
    "    model_name_or_path=MODEL_ID,\n",
    "    data_dir=DATA_DIR,\n",
    "    competition_name=COMPETITION_ID,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_batch_size=TEST_BATCH_SIZE,\n",
    "    num_workers=2,\n",
    "    max_seq_length=512,\n",
    "    val_split_percentage=0.0,\n",
    "    filter_max_words = None,  # Increase significantly from 100\n",
    "    filter_max_nonalpha_ratio = None, # Slighatly relax non-alpha ratio too\n",
    "    apply_augmentation = True\n",
    ")\n",
    "\n",
    "data_module.prepare_data() # Downloads HF data and competition data if needed\n",
    "data_module.setup() # Sets up train, val, and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if data_module.predict_dataset:\n",
    "        test_data_for_df = []\n",
    "        # Iterate through the custom dataset using __getitem__\n",
    "        num_samples_to_show = min(8000, len(data_module.predict_dataset))\n",
    "        for i in range(num_samples_to_show):\n",
    "            sample = data_module.predict_dataset[i] # Fetches the dictionary item\n",
    "            text = data_module.tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "            # Ensure 'index' exists, otherwise use a placeholder like None or -1\n",
    "            original_index = sample.get('index', None)\n",
    "            test_data_for_df.append({'Decoded Text': text, 'Original Index': original_index})\n",
    "\n",
    "        test_df = pd.DataFrame(test_data_for_df)\n",
    "    else:\n",
    "        print(\"Competition test dataset not loaded or empty.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying competition test samples: {e}\")\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c63d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Model / tokenizer ──────────────────────────────────────────────────────────\n",
    "def load_model_and_tokenizer(base_model_name, adapter_path, num_labels, device):\n",
    "    log.info(\"Loading tokenizer and base model\")\n",
    "    tok = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name, num_labels=num_labels, return_dict=True\n",
    "    )\n",
    "\n",
    "    log.info(\"Building PEFT wrapper\")\n",
    "    model = PeftModel.from_pretrained(base, adapter_path, is_trainable=False)\n",
    "\n",
    "    # manual weight load (robust to .safetensors / .bin)\n",
    "    w_path = (\n",
    "        os.path.join(adapter_path, \"adapter_model.safetensors\")\n",
    "        if os.path.exists(os.path.join(adapter_path, \"adapter_model.safetensors\"))\n",
    "        else os.path.join(adapter_path, \"adapter_model.bin\")\n",
    "    )\n",
    "    state = load_file(w_path) if w_path.endswith(\".safetensors\") else torch.load(w_path, map_location=\"cpu\")\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    if missing or unexpected:\n",
    "        log.warning(f\"Missing: {len(missing)}, unexpected: {len(unexpected)} keys\")\n",
    "\n",
    "    model.to(device).eval()\n",
    "    return model, tok\n",
    "\n",
    "# ── Device helper ──────────────────────────────────────────────────────────────\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        log.info(f\"CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda\")\n",
    "    log.info(\"CPU selected\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# ── Inference ------------------------------------------------------------------\n",
    "def run_inference(model, tokenizer, loader, device):\n",
    "    preds, idxs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Predict\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "            logits = model(**inputs).logits\n",
    "            preds.extend(logits.argmax(-1).cpu().tolist())\n",
    "            idxs.extend(batch[\"index\"].tolist())\n",
    "    return idxs, preds\n",
    "\n",
    "# ── CSV helpers ────────────────────────────────────────────────────────────────\n",
    "def save_predictions(idxs, preds, path):\n",
    "    df = pd.DataFrame({\"ID\": idxs, \"Label\": preds}).sort_values(\"ID\")\n",
    "    df.to_csv(path, index=False)\n",
    "    log.info(f\"Saved submission → {path}  ({len(df)} rows)\")\n",
    "    return df\n",
    "\n",
    "def merge_with_test_data(test_df, sub_df):\n",
    "    df = (\n",
    "        test_df.merge(sub_df, left_on=\"Original Index\", right_on=\"ID\")\n",
    "        .drop(columns=[\"Original Index\", \"ID\"])\n",
    "        .rename(columns={\"Label\": \"Predicted Label\"})\n",
    "    )\n",
    "    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "    df[\"Predicted Label\"] = df[\"Predicted Label\"].map(label_map)\n",
    "    return df\n",
    "\n",
    "# ── Pipeline ───────────────────────────────────────────────────────────────────\n",
    "def inference_pipeline(output_dir, adapter_path, num_labels, eval_metrics, test_df, data_module):\n",
    "    device = get_device()\n",
    "    acc = eval_metrics.get(\"eval_accuracy\", 0.0)\n",
    "    sub_path = os.path.join(output_dir, f\"submission_acc_new_{acc:.4f}.csv\")\n",
    "    view_path = os.path.join(output_dir, f\"view_df_acc_new_{acc:.4f}.csv\")\n",
    "\n",
    "    model, tok = load_model_and_tokenizer(\"roberta-base\", adapter_path, num_labels, device)\n",
    "    loader = data_module.get_competition_test_loader()\n",
    "\n",
    "    idxs, preds = run_inference(model, tok, loader, device)\n",
    "    sub_df = save_predictions(idxs, preds, sub_path)\n",
    "\n",
    "    if test_df is not None:\n",
    "        view_df = merge_with_test_data(test_df, sub_df)\n",
    "        view_df.to_csv(view_path, index=False)\n",
    "        log.info(f\"Saved view_df → {view_path}\")\n",
    "        return view_df\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33746f6e",
   "metadata": {},
   "source": [
    "# Reproduce Final Kaggle Submission Results Using Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir  = \"./results/713ce1e9/results\"\n",
    "adapter_dir = \"./results/713ce1e9/trained_adapters\" \n",
    "\n",
    "view_df = inference_pipeline(\n",
    "        output_dir=output_dir,\n",
    "        num_labels=4,\n",
    "        adapter_path=adapter_dir,\n",
    "        eval_metrics={},\n",
    "        test_df=test_df,\n",
    "        data_module=data_module,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
